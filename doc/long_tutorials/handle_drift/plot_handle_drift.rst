
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "long_tutorials/handle_drift/plot_handle_drift.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_long_tutorials_handle_drift_plot_handle_drift.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_long_tutorials_handle_drift_plot_handle_drift.py:


Handle motion/drift with spikeinterface NEW
===========================================

When running *in vivo* electrophysiology recordings, movement of the probe is
an inevitability, especially when the subjects are not head-fixed. SpikeInterface
includes a number of popular methods to compensate for probe motion during the
preprocessing step.

### What is drift and where does it come from?

Movement of the probe means that the spikes recorded on the probe 'drift' along it.
Typically, this motion is vertical along the probe (along the 'y' axis) which
manifests as the units moving long the probe in space.

All common motion-correction methods address this vertical drift. Horizontal ('x')
or forward/backwards ('z') motion, that would appear as the amplitude of a unit
changing over time, are much harder to model and not handled in available motion-correction algorithms.
Fortunately, vertical drift is the most common form of motion as the probe is
more likely to move along the path it was inserted, rather than in other directions
where it is buffeted against the brain.

Vertical drift can come in two forms, 'rigid' and 'non-rigid'. Rigid drift
is drift caused by movement of the entire probe and the motion is the
same for all points along the probe. Non-rigid drift is instead caused by
local movement of parts of the brain along the probe, and can affect
the recording at only certain points along the probe.

### How SpikeInterface handles drift

Spikeinterface offers a very flexible framework to handle drift as a
preprocessing step. In this tutorial we will cover the three main
drift-correction algorithms implemented in SpikeInterface with
a focus on running the methods and interpreting the output. For
more information on the theory and implementation of these methods,
see the :ref:`motion_correction` section of the documentation.

### The drift correction steps

The easiest way to run drift correction in SpikeInterface is with the
high-level :py:func:`~spikeinterface.preprocessing.correct_motion()` function.
This function takes a preprocessed recording as input and then internally runs
several steps and returns a lazy recording that interpolates the traces on-the-fly
to compensate for the motion.

The
:py:func:`~spikeinterface.preprocessing.correct_motion()`
function provides a convenient wrapper around a number of sub-functions
that together implement the full drift correction algorithm.

Internally this function runs the following steps:

| **1.** ``localize_peaks()``
| **2.** ``select_peaks()`` (optional)
| **3.** ``estimate_motion()``
| **4.** ``interpolate_motion()``

All these sub-steps have many parameters which dictate the
speed and effectiveness of motion correction. As such, `correct_motion`
provides three setting 'presets' which configure the motion correct
to proceed either as:

* `rigid_fast` - a fast, not particularly accurate correction assuming ridigt drift.
* `kilosort-like` - Mimics what is done in Kilosort (REF)
* `nonrigid_accurate` - A decentralised drift correction, introduced by the Paninski group (REF)

**Now, let's dive into running motion correction with these three
methods on a simulated dataset and interpreting the output.

.. GENERATED FROM PYTHON SOURCE LINES 75-77

.. seealso::
    hello world

.. GENERATED FROM PYTHON SOURCE LINES 77-84

.. code-block:: Python


    from pathlib import Path
    import matplotlib.pyplot as plt
    import numpy as np
    import shutil
    import spikeinterface.full as si








.. GENERATED FROM PYTHON SOURCE LINES 85-100

.. code-block:: Python

    from spikeinterface.extractors import toy_example
    from spikeinterface.generation.drifting_generator import generate_drifting_recording

    # TODO: add a note that it must be run in a if __name__ == "__main__" block.
    # TODO: is there currently any way to compute accuracy of method based on
    # drift-corrected vs. original static recording?

    _, raw_rec, _ = generate_drifting_recording(
        num_units=50, # 300,
        duration=100, # 1000
        generate_sorting_kwargs=dict(firing_rates=(15, 25), refractory_period_ms=4.0),
        seed=42,
    )
    print(raw_rec)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    InjectDriftingTemplatesRecording: 128 channels - 30.0kHz - 1 segments - 3,000,000 samples
                                      100.00s (1.67 minutes) - float32 dtype - 1.43 GiB




.. GENERATED FROM PYTHON SOURCE LINES 101-104

We preprocess the recording with bandpass filter and a common median reference.
Note, that it is better to not whiten the recording before motion estimation
to get a better estimate of peak locations!

.. GENERATED FROM PYTHON SOURCE LINES 104-115

.. code-block:: Python


    def preprocess_chain(rec):
        rec = si.bandpass_filter(rec, freq_min=300.0, freq_max=6000.0)
        rec = si.common_reference(rec, reference="global", operator="median")
        return rec


    rec = preprocess_chain(raw_rec)

    job_kwargs = dict(n_jobs=40, chunk_duration="1s", progress_bar=True)








.. GENERATED FROM PYTHON SOURCE LINES 116-125

Run motion correction with one function!
----------------------------------------

Correcting for drift is easy! You just need to run a single function.
We will try this function with 3 presets.

Internally a preset is a dictionary of dictionaries containing all parameters for every steps.

Here we also save the motion correction results into a folder to be able to load them later.

.. GENERATED FROM PYTHON SOURCE LINES 125-132

.. code-block:: Python


    # internally, we can explore a preset like this
    # every parameter can be overwritten at runtime
    from spikeinterface.preprocessing.motion import motion_options_preset

    print(motion_options_preset["kilosort_like"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'doc': 'Mimic the drift correction of kilosort (grid_convolution + iterative_template)', 'detect_kwargs': {'method': 'locally_exclusive', 'peak_sign': 'neg', 'detect_threshold': 8.0, 'exclude_sweep_ms': 0.1, 'radius_um': 50}, 'select_kwargs': {}, 'localize_peaks_kwargs': {'method': 'grid_convolution', 'radius_um': 40.0, 'upsampling_um': 5.0, 'weight_method': {'mode': 'gaussian_2d', 'sigma_list_um': array([ 5., 10., 15., 20., 25.])}, 'sigma_ms': 0.25, 'margin_um': 30.0, 'prototype': None, 'percentile': 5.0}, 'estimate_motion_kwargs': {'method': 'iterative_template', 'bin_duration_s': 2.0, 'rigid': False, 'win_step_um': 50.0, 'win_sigma_um': 150.0, 'margin_um': 0, 'win_shape': 'rect'}, 'interpolate_motion_kwargs': {'border_mode': 'force_extrapolate', 'spatial_interpolation_method': 'kriging', 'sigma_um': 20.0, 'p': 2}}




.. GENERATED FROM PYTHON SOURCE LINES 133-134

lets try theses 3 presets

.. GENERATED FROM PYTHON SOURCE LINES 134-137

.. code-block:: Python

    some_presets = ("rigid_fast", "kilosort_like", "nonrigid_accurate")
    results = {preset: {} for preset in some_presets}  # TODO: RENAME VAR








.. GENERATED FROM PYTHON SOURCE LINES 138-139

and compute motion with 3 presets

.. GENERATED FROM PYTHON SOURCE LINES 139-148

.. code-block:: Python


    for preset in some_presets:
        print("Computing with", preset)

        recording_corrected, motion_info = si.correct_motion(  # TODO: RECORDING_CORRECTED UNUSED
            rec, preset=preset,  output_motion_info=True, **job_kwargs
        )
        results[preset]["motion_info"] = motion_info





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Computing with rigid_fast
    detect and localize:   0%|          | 0/100 [00:00<?, ?it/s]    detect and localize:  12%|█▏        | 12/100 [00:00<00:04, 20.07it/s]    detect and localize:  15%|█▌        | 15/100 [00:00<00:06, 14.13it/s]    detect and localize:  21%|██        | 21/100 [00:01<00:03, 19.85it/s]    detect and localize:  25%|██▌       | 25/100 [00:01<00:04, 17.16it/s]    detect and localize:  30%|███       | 30/100 [00:01<00:03, 20.38it/s]    detect and localize:  33%|███▎      | 33/100 [00:01<00:03, 16.88it/s]    detect and localize:  38%|███▊      | 38/100 [00:02<00:03, 19.99it/s]    detect and localize:  41%|████      | 41/100 [00:02<00:03, 16.23it/s]    detect and localize:  46%|████▌     | 46/100 [00:02<00:02, 19.89it/s]    detect and localize:  49%|████▉     | 49/100 [00:02<00:03, 15.71it/s]    detect and localize:  54%|█████▍    | 54/100 [00:02<00:02, 19.33it/s]    detect and localize:  57%|█████▋    | 57/100 [00:03<00:02, 16.25it/s]    detect and localize:  61%|██████    | 61/100 [00:03<00:02, 19.26it/s]    detect and localize:  64%|██████▍   | 64/100 [00:03<00:01, 20.26it/s]    detect and localize:  67%|██████▋   | 67/100 [00:03<00:02, 16.11it/s]    detect and localize:  70%|███████   | 70/100 [00:03<00:01, 17.32it/s]    detect and localize:  73%|███████▎  | 73/100 [00:04<00:01, 17.65it/s]    detect and localize:  75%|███████▌  | 75/100 [00:04<00:01, 15.38it/s]    detect and localize:  79%|███████▉  | 79/100 [00:04<00:01, 18.90it/s]    detect and localize:  82%|████████▏ | 82/100 [00:04<00:01, 16.51it/s]    detect and localize:  86%|████████▌ | 86/100 [00:04<00:00, 18.13it/s]    detect and localize:  89%|████████▉ | 89/100 [00:04<00:00, 19.54it/s]    detect and localize:  92%|█████████▏| 92/100 [00:05<00:00, 16.91it/s]    detect and localize:  95%|█████████▌| 95/100 [00:05<00:00, 18.83it/s]    detect and localize:  98%|█████████▊| 98/100 [00:05<00:00, 18.06it/s]    detect and localize: 100%|██████████| 100/100 [00:05<00:00, 15.62it/s]    detect and localize: 100%|██████████| 100/100 [00:05<00:00, 17.62it/s]
    Computing with kilosort_like
    detect and localize:   0%|          | 0/100 [00:00<?, ?it/s]    detect and localize:  12%|█▏        | 12/100 [00:00<00:04, 17.80it/s]    detect and localize:  14%|█▍        | 14/100 [00:01<00:06, 12.77it/s]    detect and localize:  20%|██        | 20/100 [00:01<00:04, 19.46it/s]    detect and localize:  24%|██▍       | 24/100 [00:01<00:03, 20.39it/s]    detect and localize:  27%|██▋       | 27/100 [00:01<00:04, 17.29it/s]    detect and localize:  30%|███       | 30/100 [00:01<00:04, 17.27it/s]    detect and localize:  33%|███▎      | 33/100 [00:02<00:04, 14.75it/s]    detect and localize:  36%|███▌      | 36/100 [00:02<00:04, 15.76it/s]    detect and localize:  41%|████      | 41/100 [00:02<00:03, 15.03it/s]    detect and localize:  43%|████▎     | 43/100 [00:02<00:03, 15.16it/s]    detect and localize:  48%|████▊     | 48/100 [00:02<00:02, 20.06it/s]    detect and localize:  51%|█████     | 51/100 [00:03<00:03, 14.32it/s]    detect and localize:  56%|█████▌    | 56/100 [00:03<00:02, 19.26it/s]    detect and localize:  59%|█████▉    | 59/100 [00:03<00:02, 14.52it/s]    detect and localize:  63%|██████▎   | 63/100 [00:03<00:02, 18.10it/s]    detect and localize:  66%|██████▌   | 66/100 [00:04<00:02, 13.52it/s]    detect and localize:  71%|███████   | 71/100 [00:04<00:01, 18.03it/s]    detect and localize:  74%|███████▍  | 74/100 [00:04<00:01, 13.87it/s]    detect and localize:  78%|███████▊  | 78/100 [00:04<00:01, 17.36it/s]    detect and localize:  81%|████████  | 81/100 [00:05<00:01, 12.93it/s]    detect and localize:  86%|████████▌ | 86/100 [00:05<00:00, 17.89it/s]    detect and localize:  89%|████████▉ | 89/100 [00:05<00:00, 13.39it/s]    detect and localize:  93%|█████████▎| 93/100 [00:05<00:00, 16.40it/s]    detect and localize:  97%|█████████▋| 97/100 [00:06<00:00, 15.69it/s]    detect and localize: 100%|██████████| 100/100 [00:06<00:00, 15.80it/s]    detect and localize: 100%|██████████| 100/100 [00:06<00:00, 16.04it/s]
    Computing with nonrigid_accurate
    detect and localize:   0%|          | 0/100 [00:00<?, ?it/s]    detect and localize:  10%|█         | 10/100 [00:00<00:06, 14.91it/s]    detect and localize:  12%|█▏        | 12/100 [00:01<00:17,  5.06it/s]    detect and localize:  13%|█▎        | 13/100 [00:02<00:20,  4.29it/s]    detect and localize:  20%|██        | 20/100 [00:03<00:15,  5.14it/s]    detect and localize:  23%|██▎       | 23/100 [00:03<00:12,  6.03it/s]    detect and localize:  24%|██▍       | 24/100 [00:04<00:12,  6.11it/s]    detect and localize:  25%|██▌       | 25/100 [00:04<00:12,  5.98it/s]    detect and localize:  26%|██▌       | 26/100 [00:04<00:11,  6.36it/s]    detect and localize:  28%|██▊       | 28/100 [00:05<00:18,  3.90it/s]    detect and localize:  31%|███       | 31/100 [00:05<00:15,  4.50it/s]    detect and localize:  33%|███▎      | 33/100 [00:05<00:12,  5.28it/s]    detect and localize:  36%|███▌      | 36/100 [00:06<00:15,  4.03it/s]    detect and localize:  39%|███▉      | 39/100 [00:07<00:12,  4.87it/s]    detect and localize:  40%|████      | 40/100 [00:07<00:11,  5.21it/s]    detect and localize:  41%|████      | 41/100 [00:07<00:10,  5.43it/s]    detect and localize:  43%|████▎     | 43/100 [00:07<00:08,  6.89it/s]    detect and localize:  44%|████▍     | 44/100 [00:08<00:16,  3.49it/s]    detect and localize:  45%|████▌     | 45/100 [00:08<00:13,  4.01it/s]    detect and localize:  47%|████▋     | 47/100 [00:09<00:11,  4.44it/s]    detect and localize:  48%|████▊     | 48/100 [00:09<00:11,  4.42it/s]    detect and localize:  49%|████▉     | 49/100 [00:09<00:10,  4.67it/s]    detect and localize:  52%|█████▏    | 52/100 [00:10<00:11,  4.32it/s]    detect and localize:  53%|█████▎    | 53/100 [00:10<00:10,  4.39it/s]    detect and localize:  55%|█████▌    | 55/100 [00:10<00:10,  4.41it/s]    detect and localize:  56%|█████▌    | 56/100 [00:11<00:08,  4.90it/s]    detect and localize:  59%|█████▉    | 59/100 [00:11<00:05,  6.85it/s]    detect and localize:  60%|██████    | 60/100 [00:12<00:10,  3.74it/s]    detect and localize:  61%|██████    | 61/100 [00:12<00:09,  4.24it/s]    detect and localize:  63%|██████▎   | 63/100 [00:12<00:09,  3.73it/s]    detect and localize:  67%|██████▋   | 67/100 [00:13<00:05,  6.45it/s]    detect and localize:  69%|██████▉   | 69/100 [00:14<00:07,  4.01it/s]    detect and localize:  71%|███████   | 71/100 [00:14<00:06,  4.15it/s]    detect and localize:  72%|███████▏  | 72/100 [00:14<00:07,  3.90it/s]    detect and localize:  75%|███████▌  | 75/100 [00:14<00:04,  6.01it/s]    detect and localize:  77%|███████▋  | 77/100 [00:15<00:04,  5.25it/s]    detect and localize:  78%|███████▊  | 78/100 [00:15<00:04,  5.40it/s]    detect and localize:  79%|███████▉  | 79/100 [00:15<00:04,  4.76it/s]    detect and localize:  80%|████████  | 80/100 [00:16<00:04,  4.90it/s]    detect and localize:  83%|████████▎ | 83/100 [00:16<00:02,  7.51it/s]    detect and localize:  84%|████████▍ | 84/100 [00:16<00:03,  5.00it/s]    detect and localize:  85%|████████▌ | 85/100 [00:16<00:02,  5.41it/s]    detect and localize:  87%|████████▋ | 87/100 [00:17<00:02,  4.95it/s]    detect and localize:  89%|████████▉ | 89/100 [00:17<00:01,  6.10it/s]    detect and localize:  91%|█████████ | 91/100 [00:17<00:01,  6.83it/s]    detect and localize:  92%|█████████▏| 92/100 [00:18<00:01,  4.40it/s]    detect and localize:  95%|█████████▌| 95/100 [00:18<00:00,  5.41it/s]    detect and localize:  97%|█████████▋| 97/100 [00:19<00:00,  5.96it/s]    detect and localize: 100%|██████████| 100/100 [00:19<00:00,  6.27it/s]    detect and localize: 100%|██████████| 100/100 [00:19<00:00,  5.14it/s]




.. GENERATED FROM PYTHON SOURCE LINES 149-174

Plot the results
----------------

We load back the results and use the widgets module to explore the estimated drift motion.

For all methods we have 4 plots:
  * **top left:** time vs estimated peak depth
  * **top right:** time vs peak depth after motion correction
  * **bottom left:** the average motion vector across depths and all motion across spatial depths for non-rigid estimation)
  * **bottom right:** if motion correction is non rigid, the motion vector across depths is plotted as a map, with the color code representing the motion in micrometers.

A few comments on the figures:
  * The preset **'rigid_fast'** has only one motion vector for the entire probe because it is a "rigid" case.
    The motion amplitude is globally underestimated because it averages across depths.
    However, the corrected peaks are flatter than the non-corrected ones, so the job is partially done.
    The big jump at=600s when the probe start moving is recovered quite well.
  * The preset **kilosort_like** gives better results because it is a non-rigid case.
    The motion vector is computed for different depths.
    The corrected peak locations are flatter than the rigid case.
    The motion vector map is still be a bit noisy at some depths (e.g around 1000um).
  * The preset **nonrigid_accurate** seems to give the best results on this recording.
    The motion vector seems less noisy globally, but it is not "perfect" (see at the top of the probe 3200um to 3800um).
    Also note that in the first part of the recording before the imposed motion (0-600s) we clearly have a non-rigid motion:
    the upper part of the probe (2000-3000um) experience some drifts, but the lower part (0-1000um) is relatively stable.
    The method defined by this preset is able to capture this.

.. GENERATED FROM PYTHON SOURCE LINES 174-188

.. code-block:: Python


    for preset in some_presets:
        fig = plt.figure(figsize=(7, 7))
        si.plot_motion_info(
            results[preset]["motion_info"],
            recording=rec,
            figure=fig,
            depth_lim=(400, 600),
            color_amplitude=True,
            amplitude_cmap="inferno",
            scatter_decimate=10,
        )
        fig.suptitle(f"{preset=}")




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_001.png
         :alt: preset='rigid_fast', Peak depth, Corrected peak depth, Motion vectors
         :srcset: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_002.png
         :alt: preset='kilosort_like', Peak depth, Corrected peak depth, Motion vectors, Motion vectors
         :srcset: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_002.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_003.png
         :alt: preset='nonrigid_accurate', Peak depth, Corrected peak depth, Motion vectors, Motion vectors
         :srcset: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_003.png
         :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 189-207

Plot peak localization
----------------------

We can also use the internal extra results (peaks and peaks location) to check if putative
clusters have a lower spatial spread after the motion correction.

Here we plot the estimated peak locations (left) and the corrected peak locations
(on right) on top of the probe.
The color codes for the peak amplitudes.

We can see here that some clusters seem to be more compact on the 'y' axis, especially
for the preset "nonrigid_accurate".

Be aware that there are two ways to correct for the motion:
  1. Interpolate traces and detect/localize peaks again  (`interpolate_recording()`)
  2. Compensate for drifts directly on peak locations (`correct_motion_on_peaks()`)

Case 1 is used before running a spike sorter and the case 2 is used here to display the results.

.. GENERATED FROM PYTHON SOURCE LINES 207-222

.. code-block:: Python


    from spikeinterface.sortingcomponents.motion_interpolation import correct_motion_on_peaks
    from spikeinterface.widgets import plot_peaks_on_probe

    peaks = []
    peak_locations = []
    for preset in some_presets:

        motion_info = results[preset]["motion_info"]

        peaks.append(motion_info["peaks"])
        peak_locations.append(motion_info["peak_locations"])

    widget = plot_peaks_on_probe(rec, peaks, peak_locations, ylim=(200,800))
    [widget.axes[idx].set_title(subtitle) for idx, subtitle in enumerate(some_presets)]



.. image-sg:: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_004.png
   :alt: Peaks on Probe Plot, rigid_fast, kilosort_like, nonrigid_accurate
   :srcset: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    [Text(0.5, 1.0, 'rigid_fast'), Text(0.5, 1.0, 'kilosort_like'), Text(0.5, 1.0, 'nonrigid_accurate')]



.. GENERATED FROM PYTHON SOURCE LINES 223-228

Accuracy and Run Times
----------------------

Presets and related methods have differents accuracies but also computation speeds.
It is good to have this in mind!

.. GENERATED FROM PYTHON SOURCE LINES 228-246

.. code-block:: Python


    # run_times = []
    for preset in some_presets:

        # run_times.append(results[preset]["motion_info"]["run_times"])
        print(preset)
        print(results[preset]["motion_info"]["run_times"])
    if False:
        keys = run_times[0].keys()

        bottom = np.zeros(len(run_times))
        fig, ax = plt.subplots()
        for k in keys:
            rtimes = np.array([rt[k] for rt in run_times])
            if np.any(rtimes > 0.0):
                ax.bar(some_presets, rtimes, bottom=bottom, label=k)
            bottom += rtimes
        ax.legend()




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    rigid_fast
    {'detect_and_localize': 15.972179249976762, 'estimate_motion': 0.0345750420819968}
    kilosort_like
    {'detect_and_localize': 16.28575074998662, 'estimate_motion': 0.05235712497960776}
    nonrigid_accurate
    {'detect_and_localize': 29.989600916975178, 'estimate_motion': 52.08806195796933}





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 59.534 seconds)


.. _sphx_glr_download_long_tutorials_handle_drift_plot_handle_drift.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_handle_drift.ipynb <plot_handle_drift.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_handle_drift.py <plot_handle_drift.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
