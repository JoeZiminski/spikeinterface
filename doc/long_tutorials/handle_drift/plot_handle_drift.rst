
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "long_tutorials/handle_drift/plot_handle_drift.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_long_tutorials_handle_drift_plot_handle_drift.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_long_tutorials_handle_drift_plot_handle_drift.py:


===========================================
Handle motion/drift with spikeinterface NEW
===========================================

When running *in vivo* electrophysiology recordings, movement of the probe is
an inevitability, especially when the subjects are not head-fixed. SpikeInterface
includes a number of popular methods to compensate for probe motion during the
preprocessing step.

------------------------------------------
What is drift and where does it come from?
------------------------------------------

Movement of the probe means that the spikes recorded on the probe 'drift' along it.
Typically, this motion is vertical along the probe (along the 'y' axis) which
manifests as the units moving long the probe in space.

All common motion-correction methods address this vertical drift. Horizontal ('x')
or forward/backwards ('z') motion, that would appear as the amplitude of a unit
changing over time, are much harder to model and not handled in available motion-correction algorithms.
Fortunately, vertical drift is the most common form of motion as the probe is
more likely to move along the path it was inserted, rather than in other directions
where it is buffeted against the brain.

Vertical drift can come in two forms, 'rigid' and 'non-rigid'. Rigid drift
is drift caused by movement of the entire probe and the motion is the
same for all points along the probe. Non-rigid drift is instead caused by
local movement of parts of the brain along the probe, and can affect
the recording at only certain points along the probe.

------------------------------------------
How SpikeInterface handles drift
------------------------------------------

Spikeinterface offers a very flexible framework to handle drift as a
preprocessing step. In this tutorial we will cover the three main
drift-correction algorithms implemented in SpikeInterface with
a focus on running the methods and interpreting the output. For
more information on the theory and implementation of these methods,
see the :ref:`motion_correction` section of the documentation.

------------------------------------------
The drift correction steps
------------------------------------------

The easiest way to run drift correction in SpikeInterface is with the
high-level :py:func:`~spikeinterface.preprocessing.correct_motion()` function.
This function takes a preprocessed recording as input and then internally runs
several steps and returns a lazy recording that interpolates the traces on-the-fly
to compensate for the motion.

The
:py:func:`~spikeinterface.preprocessing.correct_motion()`
function provides a convenient wrapper around a number of sub-functions
that together implement the full drift correction algorithm.

Internally this function runs the following steps:

| **1.** ``localize_peaks()``
| **2.** ``select_peaks()`` (optional)
| **3.** ``estimate_motion()``
| **4.** ``interpolate_motion()``

All these sub-steps have many parameters which dictate the
speed and effectiveness of motion correction. As such, ``correct_motion``
provides three setting 'presets' which configure the motion correct
to proceed either as:

* **rigid_fast** - a fast, not particularly accurate correction assuming ridigt drift.
* **kilosort-like** - Mimics what is done in Kilosort (REF)
* **nonrigid_accurate** - A decentralised drift correction, introduced by the Paninski group (REF)

**Now, let's dive into running motion correction with these three
methods on a simulated dataset and interpreting the output.**

.. GENERATED FROM PYTHON SOURCE LINES 79-82

.. warning::
    The below code uses multiprocessing. If you are on Windows, you may
    need to place the code within a  ``if __name__ == "__main__":`` block.

.. GENERATED FROM PYTHON SOURCE LINES 84-89

-------------------------------------------
Setting up and preprocessing the recording
-------------------------------------------

First, we will import the modules we will need for this tutorial:

.. GENERATED FROM PYTHON SOURCE LINES 89-97

.. code-block:: Python


    import matplotlib.pyplot as plt
    import spikeinterface.full as si
    from spikeinterface.generation.drifting_generator import generate_drifting_recording
    from spikeinterface.preprocessing.motion import motion_options_preset
    from spikeinterface.sortingcomponents.motion_interpolation import correct_motion_on_peaks
    from spikeinterface.widgets import plot_peaks_on_probe








.. GENERATED FROM PYTHON SOURCE LINES 98-101

Next, we will generate a synthetic drifting recording. This recording will
have 500 separate units with firing rates randomly distributed between
15 and 25 Hz. The recording will be in total 1000 seconds long.

.. GENERATED FROM PYTHON SOURCE LINES 101-128

.. code-block:: Python


    # We will create a zigzag drift pattern on the recording, starting at
    # 100 seconds and with a peak-to-peak period of 100 seconds (so we will
    # have 9 zigzags through our recording). We also add some nonlinear
    # drift in to the motion (i.e. is not the same across the entire probe).

    num_units = 50 # 500,
    duration = 100  # 1000,

    _, raw_recording, _ = generate_drifting_recording(
        num_units=num_units,
        duration=duration,
        generate_sorting_kwargs=dict(firing_rates=(15, 25), refractory_period_ms=4.0),
        seed=42,
        generate_displacement_vector_kwargs=dict(motion_list=[
                dict(
                    drift_mode="zigzag",
                    non_rigid_gradient=None, # 0.1,
                    t_start_drift=10.0,  # 100.0
                    t_end_drift=None,
                    period_s=10,  # 100
                ),
            ],
        )
    )
    print(raw_recording)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    InjectDriftingTemplatesRecording: 128 channels - 30.0kHz - 1 segments - 3,000,000 samples
                                      100.00s (1.67 minutes) - float32 dtype - 1.43 GiB




.. GENERATED FROM PYTHON SOURCE LINES 129-131

Before performing motion correction, we will **preprocess** the recording
with a bandpass filter and a common median reference.

.. GENERATED FROM PYTHON SOURCE LINES 131-135

.. code-block:: Python


    filtered_recording = si.bandpass_filter(raw_recording, freq_min=300.0, freq_max=6000.0)
    preprocessed_recording = si.common_reference(filtered_recording, reference="global", operator="median")








.. GENERATED FROM PYTHON SOURCE LINES 136-140

.. warning::
    It is better to not whiten the recording before motion estimation, as this
    will give a better estimate of the peak locations. Whitening should
    be performed after motion correction.

.. GENERATED FROM PYTHON SOURCE LINES 142-156

----------------------------------------
Run motion correction with one function!
----------------------------------------

Correcting for drift is easy! You just need to run a single function.
We will now run motion correction on our recording using the three
presets described above - **rigid_fast**, **kilosort_like** and
**nonrigid_accurate**.

Under the hood, each step, peak localisation, selection, motion estimation
and interpolation expose a lot of options, making them highly flexible.
The presets are simply a set of configurations which sets the motion
correction steps to perform as described in the original methods.
For example, we can print the full set of **kilosort_like** preset options:

.. GENERATED FROM PYTHON SOURCE LINES 156-158

.. code-block:: Python

    print(motion_options_preset["kilosort_like"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    {'doc': 'Mimic the drift correction of kilosort (grid_convolution + iterative_template)', 'detect_kwargs': {'method': 'locally_exclusive', 'peak_sign': 'neg', 'detect_threshold': 8.0, 'exclude_sweep_ms': 0.1, 'radius_um': 50}, 'select_kwargs': {}, 'localize_peaks_kwargs': {'method': 'grid_convolution', 'radius_um': 40.0, 'upsampling_um': 5.0, 'weight_method': {'mode': 'gaussian_2d', 'sigma_list_um': array([ 5., 10., 15., 20., 25.])}, 'sigma_ms': 0.25, 'margin_um': 30.0, 'prototype': None, 'percentile': 5.0}, 'estimate_motion_kwargs': {'method': 'iterative_template', 'bin_duration_s': 2.0, 'rigid': False, 'win_step_um': 50.0, 'win_sigma_um': 150.0, 'margin_um': 0, 'win_shape': 'rect'}, 'interpolate_motion_kwargs': {'border_mode': 'force_extrapolate', 'spatial_interpolation_method': 'kriging', 'sigma_um': 20.0, 'p': 2}}




.. GENERATED FROM PYTHON SOURCE LINES 159-163

Now, lets run motion correction with our three presets. We will
set the ``job_kwargs`` to parallelize the job over a number of CPU cores.
Motion correction is quite computationally intensive and so it is
very useful to run it across a high numer of jobs to speed it up.

.. GENERATED FROM PYTHON SOURCE LINES 163-176

.. code-block:: Python


    presets_to_run = ("rigid_fast", "kilosort_like", "nonrigid_accurate")

    job_kwargs = dict(n_jobs=40, chunk_duration="1s", progress_bar=True)

    results = {preset: {} for preset in presets_to_run}
    for preset in presets_to_run:

        recording_corrected, motion_info = si.correct_motion(
            preprocessed_recording, preset=preset,  output_motion_info=True, **job_kwargs
        )
        results[preset]["motion_info"] = motion_info





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    detect and localize:   0%|          | 0/100 [00:00<?, ?it/s]    detect and localize:  12%|█▏        | 12/100 [00:00<00:03, 22.15it/s]    detect and localize:  15%|█▌        | 15/100 [00:00<00:06, 13.76it/s]    detect and localize:  21%|██        | 21/100 [00:01<00:04, 18.82it/s]    detect and localize:  25%|██▌       | 25/100 [00:01<00:04, 17.15it/s]    detect and localize:  28%|██▊       | 28/100 [00:01<00:03, 18.90it/s]    detect and localize:  32%|███▏      | 32/100 [00:01<00:03, 20.83it/s]    detect and localize:  35%|███▌      | 35/100 [00:01<00:03, 16.42it/s]    detect and localize:  38%|███▊      | 38/100 [00:02<00:03, 18.08it/s]    detect and localize:  41%|████      | 41/100 [00:02<00:03, 15.16it/s]    detect and localize:  44%|████▍     | 44/100 [00:02<00:03, 16.27it/s]    detect and localize:  49%|████▉     | 49/100 [00:02<00:03, 15.49it/s]    detect and localize:  52%|█████▏    | 52/100 [00:03<00:03, 15.34it/s]    detect and localize:  57%|█████▋    | 57/100 [00:03<00:02, 15.92it/s]    detect and localize:  60%|██████    | 60/100 [00:03<00:02, 16.37it/s]    detect and localize:  65%|██████▌   | 65/100 [00:03<00:01, 17.71it/s]    detect and localize:  67%|██████▋   | 67/100 [00:03<00:01, 17.90it/s]    detect and localize:  69%|██████▉   | 69/100 [00:04<00:01, 16.45it/s]    detect and localize:  73%|███████▎  | 73/100 [00:04<00:01, 16.61it/s]    detect and localize:  75%|███████▌  | 75/100 [00:04<00:01, 17.18it/s]    detect and localize:  77%|███████▋  | 77/100 [00:04<00:01, 16.08it/s]    detect and localize:  80%|████████  | 80/100 [00:04<00:01, 18.25it/s]    detect and localize:  82%|████████▏ | 82/100 [00:04<00:01, 16.40it/s]    detect and localize:  84%|████████▍ | 84/100 [00:04<00:00, 16.18it/s]    detect and localize:  86%|████████▌ | 86/100 [00:05<00:00, 15.41it/s]    detect and localize:  89%|████████▉ | 89/100 [00:05<00:00, 16.66it/s]    detect and localize:  92%|█████████▏| 92/100 [00:05<00:00, 17.47it/s]    detect and localize:  94%|█████████▍| 94/100 [00:05<00:00, 17.30it/s]    detect and localize:  97%|█████████▋| 97/100 [00:05<00:00, 18.52it/s]    detect and localize: 100%|██████████| 100/100 [00:05<00:00, 17.43it/s]    detect and localize: 100%|██████████| 100/100 [00:05<00:00, 17.04it/s]
    detect and localize:   0%|          | 0/100 [00:00<?, ?it/s]    detect and localize:  12%|█▏        | 12/100 [00:00<00:04, 21.73it/s]    detect and localize:  15%|█▌        | 15/100 [00:00<00:06, 13.80it/s]    detect and localize:  20%|██        | 20/100 [00:01<00:04, 17.75it/s]    detect and localize:  25%|██▌       | 25/100 [00:01<00:04, 15.80it/s]    detect and localize:  28%|██▊       | 28/100 [00:01<00:04, 16.93it/s]    detect and localize:  33%|███▎      | 33/100 [00:02<00:04, 14.82it/s]    detect and localize:  36%|███▌      | 36/100 [00:02<00:03, 16.13it/s]    detect and localize:  41%|████      | 41/100 [00:02<00:04, 14.73it/s]    detect and localize:  45%|████▌     | 45/100 [00:02<00:03, 17.25it/s]    detect and localize:  49%|████▉     | 49/100 [00:03<00:03, 14.71it/s]    detect and localize:  52%|█████▏    | 52/100 [00:03<00:02, 16.58it/s]    detect and localize:  56%|█████▌    | 56/100 [00:03<00:02, 20.19it/s]    detect and localize:  59%|█████▉    | 59/100 [00:03<00:02, 14.44it/s]    detect and localize:  62%|██████▏   | 62/100 [00:03<00:02, 15.70it/s]    detect and localize:  65%|██████▌   | 65/100 [00:04<00:02, 12.64it/s]    detect and localize:  69%|██████▉   | 69/100 [00:04<00:02, 15.19it/s]    detect and localize:  73%|███████▎  | 73/100 [00:04<00:02, 13.18it/s]    detect and localize:  77%|███████▋  | 77/100 [00:04<00:01, 15.12it/s]    detect and localize:  81%|████████  | 81/100 [00:05<00:01, 13.67it/s]    detect and localize:  85%|████████▌ | 85/100 [00:05<00:00, 15.18it/s]    detect and localize:  89%|████████▉ | 89/100 [00:05<00:00, 14.21it/s]    detect and localize:  91%|█████████ | 91/100 [00:05<00:00, 14.81it/s]    detect and localize:  94%|█████████▍| 94/100 [00:06<00:00, 16.74it/s]    detect and localize:  97%|█████████▋| 97/100 [00:06<00:00, 15.89it/s]    detect and localize: 100%|██████████| 100/100 [00:06<00:00, 15.97it/s]    detect and localize: 100%|██████████| 100/100 [00:06<00:00, 15.55it/s]
    detect and localize:   0%|          | 0/100 [00:00<?, ?it/s]    detect and localize:  10%|█         | 10/100 [00:00<00:04, 18.18it/s]    detect and localize:  12%|█▏        | 12/100 [00:01<00:12,  6.79it/s]    detect and localize:  13%|█▎        | 13/100 [00:01<00:15,  5.59it/s]    detect and localize:  17%|█▋        | 17/100 [00:01<00:09,  8.98it/s]    detect and localize:  19%|█▉        | 19/100 [00:02<00:16,  4.86it/s]    detect and localize:  21%|██        | 21/100 [00:03<00:14,  5.47it/s]    detect and localize:  23%|██▎       | 23/100 [00:03<00:13,  5.61it/s]    detect and localize:  24%|██▍       | 24/100 [00:03<00:12,  5.86it/s]    detect and localize:  26%|██▌       | 26/100 [00:03<00:12,  6.12it/s]    detect and localize:  27%|██▋       | 27/100 [00:04<00:22,  3.31it/s]    detect and localize:  31%|███       | 31/100 [00:05<00:13,  5.19it/s]    detect and localize:  34%|███▍      | 34/100 [00:05<00:10,  6.08it/s]    detect and localize:  35%|███▌      | 35/100 [00:06<00:16,  4.01it/s]    detect and localize:  36%|███▌      | 36/100 [00:06<00:14,  4.31it/s]    detect and localize:  39%|███▉      | 39/100 [00:06<00:10,  5.77it/s]    detect and localize:  40%|████      | 40/100 [00:06<00:10,  5.58it/s]    detect and localize:  42%|████▏     | 42/100 [00:07<00:08,  6.96it/s]    detect and localize:  43%|████▎     | 43/100 [00:08<00:16,  3.52it/s]    detect and localize:  45%|████▌     | 45/100 [00:08<00:11,  4.86it/s]    detect and localize:  47%|████▋     | 47/100 [00:08<00:09,  5.85it/s]    detect and localize:  48%|████▊     | 48/100 [00:08<00:09,  5.53it/s]    detect and localize:  49%|████▉     | 49/100 [00:08<00:08,  5.94it/s]    detect and localize:  50%|█████     | 50/100 [00:09<00:10,  4.79it/s]    detect and localize:  51%|█████     | 51/100 [00:09<00:12,  3.78it/s]    detect and localize:  53%|█████▎    | 53/100 [00:09<00:10,  4.51it/s]    detect and localize:  55%|█████▌    | 55/100 [00:10<00:08,  5.29it/s]    detect and localize:  56%|█████▌    | 56/100 [00:10<00:09,  4.59it/s]    detect and localize:  58%|█████▊    | 58/100 [00:10<00:08,  5.19it/s]    detect and localize:  59%|█████▉    | 59/100 [00:11<00:09,  4.42it/s]    detect and localize:  60%|██████    | 60/100 [00:11<00:09,  4.19it/s]    detect and localize:  63%|██████▎   | 63/100 [00:11<00:06,  5.49it/s]    detect and localize:  64%|██████▍   | 64/100 [00:11<00:06,  5.76it/s]    detect and localize:  65%|██████▌   | 65/100 [00:11<00:05,  6.24it/s]    detect and localize:  66%|██████▌   | 66/100 [00:12<00:07,  4.35it/s]    detect and localize:  67%|██████▋   | 67/100 [00:12<00:08,  4.00it/s]    detect and localize:  68%|██████▊   | 68/100 [00:12<00:07,  4.13it/s]    detect and localize:  69%|██████▉   | 69/100 [00:13<00:06,  4.67it/s]    detect and localize:  71%|███████   | 71/100 [00:13<00:06,  4.61it/s]    detect and localize:  73%|███████▎  | 73/100 [00:13<00:04,  6.10it/s]    detect and localize:  74%|███████▍  | 74/100 [00:13<00:04,  5.69it/s]    detect and localize:  75%|███████▌  | 75/100 [00:14<00:06,  3.90it/s]    detect and localize:  76%|███████▌  | 76/100 [00:14<00:05,  4.04it/s]    detect and localize:  78%|███████▊  | 78/100 [00:14<00:04,  5.12it/s]    detect and localize:  79%|███████▉  | 79/100 [00:15<00:04,  4.66it/s]    detect and localize:  80%|████████  | 80/100 [00:15<00:03,  5.11it/s]    detect and localize:  83%|████████▎ | 83/100 [00:16<00:03,  4.43it/s]    detect and localize:  84%|████████▍ | 84/100 [00:16<00:03,  4.00it/s]    detect and localize:  87%|████████▋ | 87/100 [00:16<00:02,  4.38it/s]    detect and localize:  91%|█████████ | 91/100 [00:17<00:01,  4.77it/s]    detect and localize:  93%|█████████▎| 93/100 [00:17<00:01,  5.61it/s]    detect and localize:  95%|█████████▌| 95/100 [00:18<00:00,  5.69it/s]    detect and localize:  96%|█████████▌| 96/100 [00:18<00:00,  5.83it/s]    detect and localize:  98%|█████████▊| 98/100 [00:18<00:00,  6.74it/s]    detect and localize:  99%|█████████▉| 99/100 [00:18<00:00,  7.10it/s]    detect and localize: 100%|██████████| 100/100 [00:19<00:00,  5.10it/s]    detect and localize: 100%|██████████| 100/100 [00:19<00:00,  5.24it/s]




.. GENERATED FROM PYTHON SOURCE LINES 177-182

.. seealso::
   It is often very useful to save the output of motion correction to
   file, so they can be loaded later. This can be done by setting
   the ``folder`` argument of ``correct_motion`` to a path to save in.
   The ``motion_info`` can be loaded back with ``si.load_motion_info``.

.. GENERATED FROM PYTHON SOURCE LINES 184-207

-------------------
Plotting the results
-------------------

For all methods we have 4 plots. On the x-axis of all plots we have
the (binned time). The plots display:
  * **top left:** The estimated peak depth for every detected peaks.
  * **top right:** The estimated peak depths after motion correction.
  * **bottom left:** The average motion vector across depths and all motion across spatial depths (for non-rigid estimation).
  * **bottom right:** if motion correction is non rigid, the motion vector across depths is plotted as a map, with the color code representing the motion in micrometers.

These plots are quite complicated, so it is worth covering them in detail.
For every detected action potential in our recording, we first estimate
its depth (first panel) using a method from :py:func:`~spikeinterface.postprocessing.compute_unit_locations()`.
Then, the probe motion is estimated the location of the detected peaks are
adjusted to account for this (second panel). The motion estimation produces
a measure of how much and in what direction the probe is moving at any given
time bin (third panel). For non-rigid motion correction, the probe is divided
into subsections - the motion vectors displayed are per subsection (i.e. per
'binned spatial depth') as well as the average. On the fourth panel, we see a
more detailed representation of the motion vectors. We can see the motion plotted
as a heatmap at each binned spatial depth across all time bins. We see it captures
the zigzag pattern (alternating light and dark colors).

.. GENERATED FROM PYTHON SOURCE LINES 207-221

.. code-block:: Python


    for preset in presets_to_run:
        fig = plt.figure(figsize=(7, 7))
        si.plot_motion_info(
            results[preset]["motion_info"],
            recording=recording_corrected,  # recording only used to get the real times
            figure=fig,
            depth_lim=(400, 600),
            color_amplitude=True,
            amplitude_cmap="inferno",
            scatter_decimate=10,
        )
        fig.suptitle(f"{preset=}")




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_001.png
         :alt: preset='rigid_fast', Peak depth, Corrected peak depth, Motion vectors
         :srcset: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_002.png
         :alt: preset='kilosort_like', Peak depth, Corrected peak depth, Motion vectors, Motion vectors
         :srcset: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_002.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_003.png
         :alt: preset='nonrigid_accurate', Peak depth, Corrected peak depth, Motion vectors, Motion vectors
         :srcset: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_003.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /Users/joeziminski/git_repos/forks_/spikeinterface/src/spikeinterface/widgets/motion.py:276: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.
      ax2.set_ylim(-motion_lim, motion_lim)




.. GENERATED FROM PYTHON SOURCE LINES 222-236

A few comments on the figures:
  * The preset **'rigid_fast'** has only one motion vector for the entire probe because it is a "rigid" case.
    The motion amplitude is globally underestimated because it averages across depths.
    However, the corrected peaks are flatter than the non-corrected ones, so the job is partially done.
    The big jump at=600s when the probe start moving is recovered quite well.
  * The preset **kilosort_like** gives better results because it is a non-rigid case.
    The motion vector is computed for different depths.
    The corrected peak locations are flatter than the rigid case.
    The motion vector map is still be a bit noisy at some depths (e.g around 1000um).
  * The preset **nonrigid_accurate** seems to give the best results on this recording.
    The motion vector seems less noisy globally, but it is not "perfect" (see at the top of the probe 3200um to 3800um).
    Also note that in the first part of the recording before the imposed motion (0-600s) we clearly have a non-rigid motion:
    the upper part of the probe (2000-3000um) experience some drifts, but the lower part (0-1000um) is relatively stable.
    The method defined by this preset is able to capture this.

.. GENERATED FROM PYTHON SOURCE LINES 238-246

-------------------------------------------------
Correcting Peak Locations after Motion Correction
-------------------------------------------------

To understand how motion correction is applied to our data, it is
important to understand the spikeinterface `peak` and `peak_locations`
objects, explored further in the below dropdown.


.. GENERATED FROM PYTHON SOURCE LINES 248-262

.. dropdown:: Dropdown title

  Information about detected action potentials is represented in
  SpikeInterface is ``peaks`` and ``peak_locations`` objects. The
  ``peaks`` object is an array for every detected action potential in th
   the dataset, containing its XXX, XX, XX, XX. it is created by the
   XXXX function.

  The ``peak_locations`` is a partner object to the ``peaks`` object
  containing XXX. For every peak in ``peak`` there is a corresponding
  location in ``peak_locations``. The peak locations is estimated
  using the XXXX function. One way of correcting for motion is to
  correct these peak locations directly, using the output of
  ``si.correct_motion`` or ``si.estimate_motion``.

.. GENERATED FROM PYTHON SOURCE LINES 264-278

The result of motion correction can be applied by interpolating the
raw data to correct for the drift. Essentially, this shifts the signal
across the probe depth by, at each channel, interpolating other channels
XXXX. This is performed on the `corrected_recording` output from the
`correct_motion` channel. This is useful for continuing with
preprocessing and sorting with the corrected recording.

The other way to apply the motion correction is to the ``peaks`` and
``peaks_location`` objects directly. This is done using the function
``correct_motion_on_peaks()``. Given a set of peaks, peak locations and
the ``motion`` object output from ``correct_motion``, it will shift the
location of the peaks according to the motion estimate, outputting a new
``peak_locations`` object. This is done to plot the peak locations in
the next section.

.. GENERATED FROM PYTHON SOURCE LINES 280-284

.. warning::
   Note that the `peak_locations` output by `correct_motion`'s
   `motion_info` is the ORIGINAL (uncorrected) peak locations. To get the corrected
   peak locations, `correct_motion_on_peaks()` must be used!

.. GENERATED FROM PYTHON SOURCE LINES 284-298

.. code-block:: Python


    for preset in presets_to_run:

        motion_info = results[preset]["motion_info"]

        peaks = motion_info["peaks"]

        original_peak_locations = motion_info["peak_locations"]

        corrected_peak_locations = correct_motion_on_peaks(peaks, original_peak_locations, motion_info['motion'], recording_corrected)  # TODO: what recording to use.

        widget = plot_peaks_on_probe(recording_corrected, [peaks, peaks], [original_peak_locations, corrected_peak_locations], ylim=(300,600))
        widget.figure.suptitle(preset)




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_004.png
         :alt: rigid_fast
         :srcset: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_004.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_005.png
         :alt: kilosort_like
         :srcset: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_005.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_006.png
         :alt: nonrigid_accurate
         :srcset: /long_tutorials/handle_drift/images/sphx_glr_plot_handle_drift_006.png
         :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 299-306

-------------------------
Comparing the  Run Times
-------------------------

The different methods also have different speeds, the 'nonrigid_accurate'
requires more computation time, particulary at the ``estimate_motion`` phase,
as seen in the run times:

.. GENERATED FROM PYTHON SOURCE LINES 306-311

.. code-block:: Python


    for preset in presets_to_run:
        print(preset)
        print(results[preset]["motion_info"]["run_times"])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    rigid_fast
    {'detect_and_localize': 15.370884667150676, 'estimate_motion': 0.014450875110924244}
    kilosort_like
    {'detect_and_localize': 15.728250083047897, 'estimate_motion': 0.051764374831691384}
    nonrigid_accurate
    {'detect_and_localize': 28.611334583954886, 'estimate_motion': 55.025963000021875}




.. GENERATED FROM PYTHON SOURCE LINES 312-322

------------------------
Summary
------------------------

That's it for our overall tour of correcting motion in
SpikeInterface. If you'd like to explore more, see the API docs
for `estimate_motion()`, `interpolate_motion`() (ay others?). Remember
that correcting motion makes some assumptions on your datatype - always
output and plot the motion correction information for your recordings,
to make sure they are acting as expected!


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (2 minutes 0.428 seconds)


.. _sphx_glr_download_long_tutorials_handle_drift_plot_handle_drift.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_handle_drift.ipynb <plot_handle_drift.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_handle_drift.py <plot_handle_drift.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
