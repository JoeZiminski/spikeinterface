{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Handle motion/drift with spikeinterface NEW\n\nWhen running *in vivo* electrophysiology recordings, movement of the probe is\nan inevitability, especially when the subjects are not head-fixed. SpikeInterface\nincludes a number of popular methods to compensate for probe motion during the\npreprocessing step.\n\n## What is drift and where does it come from?\n\nMovement of the probe means that the spikes recorded on the probe 'drift' along it.\nTypically, this motion is vertical along the probe (along the 'y' axis) which\nmanifests as the units moving long the probe in space.\n\nAll common motion-correction methods address this vertical drift. Horizontal ('x')\nor forward/backwards ('z') motion, that would appear as the amplitude of a unit\nchanging over time, are much harder to model and not handled in available motion-correction algorithms.\nFortunately, vertical drift is the most common form of motion as the probe is\nmore likely to move along the path it was inserted, rather than in other directions\nwhere it is buffeted against the brain.\n\nVertical drift can come in two forms, 'rigid' and 'non-rigid'. Rigid drift\nis drift caused by movement of the entire probe and the motion is the\nsame for all points along the probe. Non-rigid drift is instead caused by\nlocal movement of parts of the brain along the probe, and can affect\nthe recording at only certain points along the probe.\n\n## How SpikeInterface handles drift\n\nSpikeinterface offers a very flexible framework to handle drift as a\npreprocessing step. In this tutorial we will cover the three main\ndrift-correction algorithms implemented in SpikeInterface with\na focus on running the methods and interpreting the output. For\nmore information on the theory and implementation of these methods,\nsee the `motion_correction` section of the documentation.\n\n## The drift correction steps\n\nThe easiest way to run drift correction in SpikeInterface is with the\nhigh-level :py:func:`~spikeinterface.preprocessing.correct_motion()` function.\nThis function takes a preprocessed recording as input and then internally runs\nseveral steps and returns a lazy recording that interpolates the traces on-the-fly\nto compensate for the motion.\n\nThe\n:py:func:`~spikeinterface.preprocessing.correct_motion()`\nfunction provides a convenient wrapper around a number of sub-functions\nthat together implement the full drift correction algorithm.\n\nInternally this function runs the following steps:\n\n| **1.** ``localize_peaks()``\n| **2.** ``select_peaks()`` (optional)\n| **3.** ``estimate_motion()``\n| **4.** ``interpolate_motion()``\n\nAll these sub-steps have many parameters which dictate the\nspeed and effectiveness of motion correction. As such, ``correct_motion``\nprovides three setting 'presets' which configure the motion correct\nto proceed either as:\n\n* **rigid_fast** - a fast, not particularly accurate correction assuming ridigt drift.\n* **kilosort-like** - Mimics what is done in Kilosort (REF)\n* **nonrigid_accurate** - A decentralised drift correction, introduced by the Paninski group (REF)\n\n**Now, let's dive into running motion correction with these three\nmethods on a simulated dataset and interpreting the output.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>The below code uses multiprocessing. If you are on Windows, you may\n    need to place the code within a  ``if __name__ == \"__main__\":`` block.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up and preprocessing the recording\n\nFirst, we will import the modules we will need for this tutorial:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport spikeinterface.full as si\nfrom spikeinterface.generation.drifting_generator import generate_drifting_recording\nfrom spikeinterface.preprocessing.motion import motion_options_preset\nfrom spikeinterface.sortingcomponents.motion_interpolation import correct_motion_on_peaks\nfrom spikeinterface.widgets import plot_peaks_on_probe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will generate a synthetic drifting recording. This recording will\nhave 500 separate units with firing rates randomly distributed between\n15 and 25 Hz. The recording will be in total 1000 seconds long.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We will create a zigzag drift pattern on the recording, starting at\n# 100 seconds and with a peak-to-peak period of 100 seconds (so we will\n# have 9 zigzags through our recording). We also add some nonlinear\n# drift in to the motion (i.e. is not the same across the entire probe).\n\nnum_units = 50 # 500,\nduration = 100  # 1000,\n\n_, raw_recording, _ = generate_drifting_recording(\n    num_units=num_units,\n    duration=duration,\n    generate_sorting_kwargs=dict(firing_rates=(15, 25), refractory_period_ms=4.0),\n    seed=42,\n    generate_displacement_vector_kwargs=dict(motion_list=[\n            dict(\n                drift_mode=\"zigzag\",\n                non_rigid_gradient=None, # 0.1,\n                t_start_drift=10.0,  # 100.0\n                t_end_drift=None,\n                period_s=10,  # 100\n            ),\n        ],\n    )\n)\nprint(raw_recording)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before performing motion correction, we will **preprocess** the recording\nwith a bandpass filter and a common median reference.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filtered_recording = si.bandpass_filter(raw_recording, freq_min=300.0, freq_max=6000.0)\npreprocessed_recording = si.common_reference(filtered_recording, reference=\"global\", operator=\"median\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>It is better to not whiten the recording before motion estimation, as this\n    will give a better estimate of the peak locations. Whitening should\n    be performed after motion correction.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run motion correction with one function!\n\nCorrecting for drift is easy! You just need to run a single function.\nWe will now run motion correction on our recording using the three\npresets described above - **rigid_fast**, **kilosort_like** and\n**nonrigid_accurate**.\n\nUnder the hood, each step, peak localisation, selection, motion estimation\nand interpolation expose a lot of options, making them highly flexible.\nThe presets are simply a set of configurations which sets the motion\ncorrection steps to perform as described in the original methods.\nFor example, we can print the full set of **kilosort_like** preset options:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(motion_options_preset[\"kilosort_like\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets run motion correction with our three presets. We will\nset the ``job_kwargs`` to parallelize the job over a number of CPU cores.\nMotion correction is quite computationally intensive and so it is\nvery useful to run it across a high numer of jobs to speed it up.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "presets_to_run = (\"rigid_fast\", \"kilosort_like\", \"nonrigid_accurate\")\n\njob_kwargs = dict(n_jobs=40, chunk_duration=\"1s\", progress_bar=True)\n\nresults = {preset: {} for preset in presets_to_run}\nfor preset in presets_to_run:\n\n    recording_corrected, motion_info = si.correct_motion(\n        preprocessed_recording, preset=preset,  output_motion_info=True, **job_kwargs\n    )\n    results[preset][\"motion_info\"] = motion_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. seealso::\n   It is often very useful to save the output of motion correction to\n   file, so they can be loaded later. This can be done by setting\n   the ``folder`` argument of ``correct_motion`` to a path to save in.\n   The ``motion_info`` can be loaded back with ``si.load_motion_info``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting the results\n\nFor all methods we have 4 plots. On the x-axis of all plots we have\nthe (binned time). The plots display:\n  * **top left:** The estimated peak depth for every detected peaks.\n  * **top right:** The estimated peak depths after motion correction.\n  * **bottom left:** The average motion vector across depths and all motion across spatial depths (for non-rigid estimation).\n  * **bottom right:** if motion correction is non rigid, the motion vector across depths is plotted as a map, with the color code representing the motion in micrometers.\n\nThese plots are quite complicated, so it is worth covering them in detail.\nFor every detected action potential in our recording, we first estimate\nits depth (first panel) using a method from :py:func:`~spikeinterface.postprocessing.compute_unit_locations()`.\nThen, the probe motion is estimated the location of the detected peaks are\nadjusted to account for this (second panel). The motion estimation produces\na measure of how much and in what direction the probe is moving at any given\ntime bin (third panel). For non-rigid motion correction, the probe is divided\ninto subsections - the motion vectors displayed are per subsection (i.e. per\n'binned spatial depth') as well as the average. On the fourth panel, we see a\nmore detailed representation of the motion vectors. We can see the motion plotted\nas a heatmap at each binned spatial depth across all time bins. We see it captures\nthe zigzag pattern (alternating light and dark colors).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for preset in presets_to_run:\n    fig = plt.figure(figsize=(7, 7))\n    si.plot_motion_info(\n        results[preset][\"motion_info\"],\n        recording=recording_corrected,  # recording only used to get the real times\n        figure=fig,\n        depth_lim=(400, 600),\n        color_amplitude=True,\n        amplitude_cmap=\"inferno\",\n        scatter_decimate=10,\n    )\n    fig.suptitle(f\"{preset=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few comments on the figures:\n  * The preset **'rigid_fast'** has only one motion vector for the entire probe because it is a \"rigid\" case.\n    The motion amplitude is globally underestimated because it averages across depths.\n    However, the corrected peaks are flatter than the non-corrected ones, so the job is partially done.\n    The big jump at=600s when the probe start moving is recovered quite well.\n  * The preset **kilosort_like** gives better results because it is a non-rigid case.\n    The motion vector is computed for different depths.\n    The corrected peak locations are flatter than the rigid case.\n    The motion vector map is still be a bit noisy at some depths (e.g around 1000um).\n  * The preset **nonrigid_accurate** seems to give the best results on this recording.\n    The motion vector seems less noisy globally, but it is not \"perfect\" (see at the top of the probe 3200um to 3800um).\n    Also note that in the first part of the recording before the imposed motion (0-600s) we clearly have a non-rigid motion:\n    the upper part of the probe (2000-3000um) experience some drifts, but the lower part (0-1000um) is relatively stable.\n    The method defined by this preset is able to capture this.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correcting Peak Locations after Motion Correction\n\nTo understand how motion correction is applied to our data, it is\nimportant to understand the spikeinterface `peak` and `peak_locations`\nobjects, explored further in the below dropdown.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. dropdown:: Dropdown title\n\n  Information about detected action potentials is represented in\n  SpikeInterface is ``peaks`` and ``peak_locations`` objects. The\n  ``peaks`` object is an array for every detected action potential in th\n   the dataset, containing its XXX, XX, XX, XX. it is created by the\n   XXXX function.\n\n  The ``peak_locations`` is a partner object to the ``peaks`` object\n  containing XXX. For every peak in ``peak`` there is a corresponding\n  location in ``peak_locations``. The peak locations is estimated\n  using the XXXX function. One way of correcting for motion is to\n  correct these peak locations directly, using the output of\n  ``si.correct_motion`` or ``si.estimate_motion``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result of motion correction can be applied by interpolating the\nraw data to correct for the drift. Essentially, this shifts the signal\nacross the probe depth by, at each channel, interpolating other channels\nXXXX. This is performed on the `corrected_recording` output from the\n`correct_motion` channel. This is useful for continuing with\npreprocessing and sorting with the corrected recording.\n\nThe other way to apply the motion correction is to the ``peaks`` and\n``peaks_location`` objects directly. This is done using the function\n``correct_motion_on_peaks()``. Given a set of peaks, peak locations and\nthe ``motion`` object output from ``correct_motion``, it will shift the\nlocation of the peaks according to the motion estimate, outputting a new\n``peak_locations`` object. This is done to plot the peak locations in\nthe next section.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Note that the `peak_locations` output by `correct_motion`'s\n   `motion_info` is the ORIGINAL (uncorrected) peak locations. To get the corrected\n   peak locations, `correct_motion_on_peaks()` must be used!</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for preset in presets_to_run:\n\n    motion_info = results[preset][\"motion_info\"]\n\n    peaks = motion_info[\"peaks\"]\n\n    original_peak_locations = motion_info[\"peak_locations\"]\n\n    corrected_peak_locations = correct_motion_on_peaks(peaks, original_peak_locations, motion_info['motion'], recording_corrected)  # TODO: what recording to use.\n\n    widget = plot_peaks_on_probe(recording_corrected, [peaks, peaks], [original_peak_locations, corrected_peak_locations], ylim=(300,600))\n    widget.figure.suptitle(preset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing the  Run Times\n\nThe different methods also have different speeds, the 'nonrigid_accurate'\nrequires more computation time, particulary at the ``estimate_motion`` phase,\nas seen in the run times:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for preset in presets_to_run:\n    print(preset)\n    print(results[preset][\"motion_info\"][\"run_times\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nThat's it for our overall tour of correcting motion in\nSpikeInterface. If you'd like to explore more, see the API docs\nfor `estimate_motion()`, `interpolate_motion`() (ay others?). Remember\nthat correcting motion makes some assumptions on your datatype - always\noutput and plot the motion correction information for your recordings,\nto make sure they are acting as expected!\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
